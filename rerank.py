"""rerank the outputs generated by the speaker
"""

import tensorflow as tf
import numpy as np
import os
import argparse
import json
import sys

import data_loader as data
import network.listener_net as net
from utils.prepare_embedding import save_concise_emb


def main():
    parser = argparse.ArgumentParser()
    # Rerank parameters
    parser.add_argument('--listener_path', type=str, default='',
                        help='path to the listener model used for reranking')
    parser.add_argument('--listener_model', type=str, default='',
                        help='model name (model-%steps) in listener_path')
    parser.add_argument('--speaker_result_path', type=str, default='',
                        help='path to the phrases result generated by a speaker')
    # Below two are useless if you set the above three arguments
    parser.add_argument('--listener_mode', type=str, default='L', help='')
    parser.add_argument('--speaker_mode', type=str, default='DS', help='')
    
    parser.add_argument('--infer_dataset', type=str, default='test', help='')
    parser.add_argument('--rerank', type=int, default=1,
                        help='whether to rerank and report user study accuracy')
    parser.add_argument('--rerank_by', type=str, default='log_prob',
                        help='prob or log_prob or sub')
    parser.add_argument('--user_study', type=int, default=0,
                        help='whether we have user study information in speaker result, 0 or 1')
    args = parser.parse_args()
    
    if args.listener_mode == 'L_fix':
        log_dir = 'reusult/listener/over_fitted/0_0/'
        listener_model_path = 'reusult/listener/over_fitted/0_0/model-fixed-10000'
    elif args.listener_mode == 'Lr_fix':
        log_dir = 'reusult/listener/over_fitted/1_0/'
        listener_model_path = 'reusult/listener/over_fitted/1_0/model-fixed-10000'
    elif args.listener_mode == 'L':
        log_dir = 'reusult/listener/0_0_reg_finetune/'
        listener_model_path = 'reusult/listener/0_0_reg_finetune/model-train-7500'
    elif args.listener_mode == 'Lr':
        log_dir = 'reusult/listener/1_0_reg_finetune/'
        listener_model_path = 'reusult/listener/1_0_reg_finetune/model-train-10000'
    elif args.listener_mode == 'DL':
        log_dir = 'reusult/listener/0_1_reg_finetune/'
        listener_model_path = 'reusult/listener/0_1_reg_finetune/model-train-7000'
    elif listener_path == '':
        log_dir = ''
        listener_model_path = ''
        print 'ERR: unknown listener mode'
        exit()

    if not args.listener_path == '':
        log_dir = args.listener_path
        listener_model_path = os.path.join(args.listener_path, args.listener_model)

    load_arguments(parser, os.path.join(log_dir, 'config.json'))
    args = parser.parse_args()
    args.log_dir = log_dir
    args.listener_model_path = listener_model_path


    if args.infer_dataset == 'test':
        args.dataset = 'test'
    if not args.rerank:
        # args.speaker_result_path = 'caption/output/%s_fixCNN_lr6_1000__2017-03-13-03/' \
        # args.speaker_result_path = 'caption/output/%s_fixCNN__2017-03-07-16/' \
        # args.speaker_result_path = 'caption/output/%s_fixCNN__2017-03-07-16/' \
        # args.speaker_result_path = 'caption/output/%s_tuneCNN_model-40000_lr6_5__2017-03-13-03/' \

        speaker_result_path = 'result/speaker/%s_tuneCNN_model-40000__2017-03-07-19/' \
                                   'infer_annotations_%s_model-40000_case0_beam10_sent10.json' \
                                   % (args.speaker_mode, args.infer_dataset)
        output_dir = 'result/rerank/norerank_%s_%s_%s' % (args.listener_mode, args.speaker_mode, args.infer_dataset)
    else:
        if args.infer_dataset == 'test':
            task_count = 100
        else:
            task_count = 50
        speaker_result_path = 'result/user_study/task_result_%s_%s_1_%d.json' \
                                   % (args.infer_dataset, args.speaker_mode, task_count)
        output_dir = 'result/rerank/rerank_%s_%s_%s_%s' \
                     % (args.listener_mode, args.speaker_mode, args.infer_dataset, args.rerank_by)
    if args.speaker_result_path == '':
        args.speaker_result_path = speaker_result_path
    train(args, args.listener_mode, args.speaker_mode, args.listener_model_path, args.speaker_result_path, output_dir)


def load_arguments(parser, json_fpath):
    with open(json_fpath) as data_file:
        arg_dict = json.load(data_file)
    for (key, value) in arg_dict.items():
        parser.add_argument('--' + key, default=value)


def train(args, listener_mode, speaker_mode, listener_model_path, speaker_result_path, output_dir):
    # save config
    print '=============='
    print 'Configurations:'
    for arg in vars(args):
        print arg + ":" + str(getattr(args, arg))
    print '=============='
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    config_f = open(output_dir + '/config.json', 'w')
    json.dump(vars(args), config_f)
    config_f.close()

    # Load Speaker result
    with open(speaker_result_path) as data_file:
        result_dataset = json.load(data_file)

    for anno in result_dataset:
        if speaker_mode == 'DS':
            sen_split1 = [None]*10
            sen_split2 = [None]*10
            for i in xrange(10):
                sent = anno['gen_texts'][i].split(' _VS ')
                if len(sent) >= 2:
                    sen_split1[i] = sent[0]
                    sen_split2[i] = sent[1]
                else:
                    sen_split1[i] = sent[0]
                    sen_split2[i] = ''
                    print 'WARNING: generated text without _VS: ' + anno['gen_texts'][i]
            anno['gen_sent1'] = sen_split1
            anno['gen_sent2'] = sen_split2
            # anno['gen_sent1'] = anno['sentences1']
            # anno['gen_sent2'] = anno['sentences2']
        else:
            anno['gen_sent1'] = anno['gen_texts']
            anno['gen_sent2'] = [''] * 10

    # Load imgs_dict
    if not args.train_img_model:
        if args.infer_dataset == 'test':
            imgs_dict_fpath = 'imgs_dict/test_dict.json'
        else:
            imgs_dict_fpath = 'imgs_dict/trainval_dict.json'
        with open(imgs_dict_fpath) as imgs_dict_file:
            imgs_dict = json.load(imgs_dict_file)
    else:
        imgs_dict = []

    # Build vocab and save word2vec embedding
    if args.word_embed_type == 'char':
        annFile_json_train = 'dataset/visdiff_train_v2.json'
        vocabulary = data.build_vocabulary(annFile_json_train, args.word_count_thresh, args.word_embed_type)
    else:
        vocabulary = np.load(args.vocab_file).item()
    if args.word_embed_type == 'word2vec':
        word2vec_npy_path = "models/word2vec_concise_"+str(args.word_count_thresh)+'.npy'
        if not tf.gfile.Exists(word2vec_npy_path):
            save_concise_emb(vocabulary, word2vec_npy_path)

    # Build data_feeder
    if args.img_model=='alexnet' or args.img_model=='vgg_16':
        img_w = img_h = 224
    elif args.img_model=='inception_v3':
        img_w = img_h = 299
    else:
        raise Exception("model type not supported: {}".format(args.img_model))

    lm = listener_mode
    t = listener_mode.split('_')
    if len(t) > 1:
        lm = t[0]
    data_val = data.DataFeeder(result_dataset, vocabulary=vocabulary, train_img_model=args.train_img_model,
                               img_dict=imgs_dict, feed_mode=lm, rand_neg=False,
                               max_length=args.max_sent_length, img_w=img_w, img_h=img_h,
                               word_embed_type=args.word_embed_type, trim_last_batch=True,
                               shuffle=False)

    with tf.Graph().as_default():

        # Create a session for running Ops on the Graph
        sess = tf.Session()

        # Setup the network
        args.mode = 'eval'  # 'train', 'eval' or 'inference'. For batch_norm, dropout, and image jittering or not
        my_net = net.ImgEmbedNet(args, vocab_length=len(vocabulary), graph=sess.graph, log_dir=args.log_dir,
                                 img_w=img_w, img_h=img_h, dataset=args.dataset)
        my_net.build()

        # Initialize the variables
        init = tf.global_variables_initializer()
        sess.run(init)

        # Load pre-trained model weights
        if args.img_model == 'inception_v3':
            scope_name = 'InceptionV3'
        elif args.img_model == 'vgg_16':
            scope_name = 'vgg_16'
        else:
            scope_name = 'Alexnet'
        if args.img_model != 'alexnet' and args.train_img_model:
            checkpoint_file = 'models/checkpoints/'+args.img_model+'.ckpt'
            img_model_variables = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope_name)
            saver = tf.train.Saver(img_model_variables)
            saver.restore(sess, checkpoint_file)

        # Load previous checkpoint with fixed CNN feature
        if listener_model_path != 'none':
            all_variables = {v for v in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)}
            other_variables = {v for v in all_variables if
                               (v not in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='optimizer')
                                and (v not in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='embeddings')))}
            if not args.train_img_model:
                other_variables = {v for v in other_variables if
                                   v not in tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=scope_name)}
            saver = tf.train.Saver(other_variables)
            saver.restore(sess, listener_model_path)

        # initialize for validation error
        start_iter_count = data_val.epoch_count
        all_prob_listener_w1 = np.empty(len(data_val.annotations))
        all_prob_listener_w2 = np.empty(len(data_val.annotations))
        all_prob_speaker = np.empty(len(data_val.annotations))
        all_pred_w1 = np.empty(len(data_val.annotations))
        all_pred_w2 = np.empty(len(data_val.annotations))
        all_logit_w1 = np.empty([len(data_val.annotations), 2])
        all_logit_w2 = np.empty([len(data_val.annotations), 2])
        # Set log path
        log_f = open(output_dir + '/rerank.txt', 'w')

        # Go through all test pairs once and save the result
        counter = 0
        while start_iter_count == data_val.epoch_count:
            val_batch = data_val.get_batch(args.batch_size)
            batch_count = len(val_batch['img1'])
            if listener_mode == 'DL':
                val_dict = {
                    my_net.img1: val_batch['img1'],
                    my_net.img2: val_batch['img2'],
                    my_net.text1: val_batch['encode_text1'],
                    my_net.text2: val_batch['encode_text2'],
                    my_net.text_len: val_batch['text_len'],
                    my_net.is_training_ph: False}
            else:  # 'L'
                val_dict = {
                    my_net.img1: val_batch['img1'],
                    my_net.img2: val_batch['img2'],
                    my_net.text1: val_batch['encode_sent1'],
                    my_net.text2: val_batch['encode_sent2'],
                    my_net.s_len1: val_batch['sent_len1'],
                    my_net.s_len2: val_batch['sent_len2'],
                    my_net.is_training_ph: False}

            acc_txt2img, pred_w1, pred_w2, prob_w1, prob_w2, logit_w1, logit_w2 = sess.run([my_net.acc_txt2img,
                    my_net.pred_w1, my_net.pred_w2, my_net.prob_w1, my_net.prob_w2, my_net.logit_w1, my_net.logit_w2],
                                                                                           feed_dict=val_dict)

            all_prob_listener_w1[counter:counter+batch_count] = prob_w1[:, 0]
            all_prob_listener_w2[counter:counter+batch_count] = prob_w2[:, 0]
            all_pred_w1[counter:counter+batch_count] = pred_w1
            all_pred_w2[counter:counter+batch_count] = pred_w1
            all_logit_w1[counter:counter+batch_count] = logit_w1
            all_logit_w2[counter:counter+batch_count] = logit_w2

            prob_speaker = val_batch['gen_probs']
            all_prob_speaker[counter:counter+batch_count] = prob_speaker

            counter += batch_count

        # Get original Speaker accuracy
        acc_l = float(np.sum(all_pred_w1)) / all_pred_w1.shape[0]
        acc_r = float(np.sum(all_pred_w2)) / all_pred_w2.shape[0]
        acc_mean = (acc_l + acc_r) / 2.0
        msg = 'Overall accuracy from listener: S1: %.4f; S2: %.4f; mean: %.4f' % (acc_l, acc_r, acc_mean)
        print msg

        log_f.write(msg + '\n')

        # change data type to float
        all_pred_w1 = all_pred_w1.astype(np.float32)

        # Judge the speaker by listener (no rerank, no human listener)
        msg = 'Top K accuracy from listener:'
        print msg
        log_f.write(msg + '\n')
        for top_k in xrange(1, 11):
            ori_sen_acc_all = np.array([])
            for i in xrange(all_prob_speaker.shape[0]/10):
                list_pred_w1 = all_pred_w1[i*10:i*10+10]
                ori_sen_acc_all = np.append(ori_sen_acc_all, list_pred_w1[:top_k])
            ori_sen_acc = sum(ori_sen_acc_all)*1.0/float(len(ori_sen_acc_all))
            msg = 'top %d:\t%.4f' % (top_k, ori_sen_acc)
            print msg
            log_f.write(msg + '\n')

        # rerank top_k result with different lambda
        if args.rerank:
            msg = 'Rerank result:'
            print msg
            log_f.write(msg + '\n')
            msg = 'top_k\torigin_acc\torigin_guess\trerank_acc\trerank_guess'
            print msg
            log_f.write(msg + '\n')
            lambda_p_all = np.linspace(0.0, 0.5, num=11)
            rerank_order = np.empty((all_prob_listener_w1.shape[0]/10, 10), dtype=np.int8)
            correctness_all = np.empty((all_prob_listener_w1.shape[0]/10, 10), dtype=np.int8)
            if args.rerank_by == 'prob':
                all_score_listener = all_prob_listener_w1
                all_score_speaker = all_prob_speaker
            elif args.rerank_by == 'log_prob':
                all_score_listener = -np.exp(all_logit_w1[:, 1] - all_logit_w1[:, 0])
                all_score_speaker = np.log(all_prob_speaker)
            else:
                all_score_listener = all_logit_w1[:, 0] - all_logit_w1[:, 1]
                all_score_speaker = np.log(all_prob_speaker)

            for top_k in xrange(1, 11):
                for lambda_i in xrange(len(lambda_p_all)):
                    # calculate joint probability
                    lambda_p = lambda_p_all[lambda_i]

                    if args.rerank_by == 'prob':
                        all_score_joint = np.multiply(np.power(all_score_listener, 1.0 - lambda_p),
                                                      np.power(all_score_speaker, lambda_p))
                    else:
                        all_score_joint = lambda_p * all_score_speaker + (1-lambda_p) * all_score_listener

                    # initial list for new ranking result
                    ori_sen_acc_all = np.array([])
                    ranked_sen_acc_all = np.array([])

                    for i in xrange(all_score_joint.shape[0]/10):
                        # get new index
                        prob_list = all_score_joint[i*10:i*10+10]
                        idx = np.argsort(-prob_list, axis=0)  # descending
                        idx_ = idx[0:top_k]
                        if args.user_study:
                            list_correctness = np.array(result_dataset[i]['is_correct'])
                        else:
                            list_correctness = np.zeros(10)
                        if top_k == 10 and lambda_i == 0:
                            rerank_order[i, :] = idx
                            correctness_all[i, :] = list_correctness

                        ori_sen_acc_all = np.append(ori_sen_acc_all, list_correctness[:top_k])
                        # ori_sen_acc_all = np.append(ori_sen_acc_all, all_pred_w2_[:top_k])
                        ranked_sen_acc_all = np.append(ranked_sen_acc_all, list_correctness[idx_])
                        # ranked_sen_acc_all = np.append(ranked_sen_acc_all, all_pred_w2_[idx_])

                    ori_sen_acc = sum(ori_sen_acc_all == 1)*1.0/float(len(ori_sen_acc_all))
                    ranked_sen_acc = sum(ranked_sen_acc_all == 1)*1.0/float(len(ranked_sen_acc_all))
                    ori_sen_unc = sum(ori_sen_acc_all == 0)*1.0/float(len(ori_sen_acc_all))
                    ranked_sen_unc = sum(ranked_sen_acc_all == 0)*1.0/float(len(ranked_sen_acc_all))
                    ori_sen_guess = ori_sen_acc + ori_sen_unc * 0.5
                    ranked_sen_guess = ranked_sen_acc + ranked_sen_unc * 0.5
                    # if lambda_i == 0:
                    #     print 'top %d:\t%.4f\t%.4f' % (top_k, ori_sen_acc, ranked_sen_acc)
                    msg = '%d\t%.4f\t%.4f\t%.4f\t%.4f\t%.4f' % (top_k, lambda_p, ori_sen_acc, ori_sen_guess,
                                                                ranked_sen_acc, ranked_sen_guess)
                    print msg
                    log_f.write(msg + '\n')

            # print to html
            html_f = open(output_dir + '/rerank.html', 'w')
            html_f.write('<table align=\'center\'>\n')
            for case_i in range(min(500, len(result_dataset))):  # range(rerank_order.shape[0]):
                img1_id = result_dataset[case_i]['img1_id']
                img2_id = result_dataset[case_i]['img2_id']
                true_s1 = result_dataset[case_i]['sentences1']
                true_s2 = result_dataset[case_i]['sentences2']
                gen_texts = np.asarray(result_dataset[case_i]['gen_texts'])[rerank_order[case_i, :]]
                feed_backs = np.array(result_dataset[case_i]['feed_backs']).astype(int)[rerank_order[case_i, :]]
                l_scores = all_score_listener[case_i*10: case_i*10+10][rerank_order[case_i, :]]
                s_scores = all_score_speaker[case_i*10: case_i*10+10][rerank_order[case_i, :]]
                listener_probs_w2 = all_prob_listener_w2[case_i*10: case_i*10+10][rerank_order[case_i, :]]
                origin_order = np.asarray(range(1, 11))[rerank_order[case_i, :]]
                _rerank_wirte_as_html(html_f, img1_id, img2_id, true_s1, true_s2, gen_texts, l_scores,
                                      listener_probs_w2, s_scores, feed_backs, origin_order)
            html_f.write('</table>\n')
            html_f.close()

        log_f.close()


def _rerank_wirte_as_html(html_f, img1_id, img2_id, true_s1, true_s2, gen_texts, l_scores, w2_probs,
                          s_scores, feed_backs, origin_order, img_dir='../../images'):
    msg = "*** image %s vs %s ***" % (img1_id, img2_id)
    html_f.write('<tr><td>' + msg + '</td></tr>\n')
    msg = '<img src=\'%s.jpg\' height=150><img src=\'%s.jpg\' height=150>' \
          % (os.path.join(img_dir, img1_id), os.path.join(img_dir, img2_id))
    html_f.write('<tr><td>' + msg + '</td></tr>\n')
    msg = "Ground Truth:"
    html_f.write('<tr><td>' + msg + '</td></tr>\n')
    for i in range(5):
        msg = '  %s _VS %s' % (true_s1[i], true_s2[i])
        html_f.write('<tr><td>' + msg + '</td></tr>\n')
    msg = "Generated (ranked by listener):"
    html_f.write('<tr><td>' + msg + '</td></tr>\n')
    for i in range(len(gen_texts)):
        # msg = "%s[L %02d %.4f and %.4f][S %02d %.4f] %s" \
        #       % (str(feed_backs[i]), i + 1, l_scores[i], w2_probs[i], origin_order[i], s_scores[i], gen_texts[i])
        msg = "%s[L %02d %.4g][S %02d %.4f] %s" \
              % (str(feed_backs[i]), i + 1, l_scores[i], origin_order[i], s_scores[i], gen_texts[i])
        html_f.write('<tr><td>' + msg + '</td></tr>\n')
    html_f.write('<tr> <td colspan="4" bgcolor="#009966" height="3">&nbsp;</td> </tr>\n')

if __name__ == '__main__':
    main()
